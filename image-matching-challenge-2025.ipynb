{"metadata":{"kernelspec":{"language":"python","display_name":"Python 3","name":"python3"},"language_info":{"name":"python","version":"3.10.12","mimetype":"text/x-python","codemirror_mode":{"name":"ipython","version":3},"pygments_lexer":"ipython3","nbconvert_exporter":"python","file_extension":".py"},"kaggle":{"accelerator":"nvidiaTeslaT4","dataSources":[{"sourceId":91498,"databundleVersionId":11655853,"sourceType":"competition"},{"sourceId":7884485,"sourceType":"datasetVersion","datasetId":4628051},{"sourceId":11924468,"sourceType":"datasetVersion","datasetId":6988459},{"sourceId":4534,"sourceType":"modelInstanceVersion","modelInstanceId":3326,"modelId":986},{"sourceId":17191,"sourceType":"modelInstanceVersion","modelInstanceId":14317,"modelId":21716},{"sourceId":17555,"sourceType":"modelInstanceVersion","modelInstanceId":14611,"modelId":22086}],"dockerImageVersionId":30918,"isInternetEnabled":false,"language":"python","sourceType":"notebook","isGpuEnabled":true}},"nbformat_minor":4,"nbformat":4,"cells":[{"cell_type":"code","source":"# Install dependencies and copy model weights to run the notebook without internet access when submitting to the competition.\n\n!pip install --no-index /kaggle/input/imc2024-packages-lightglue-rerun-kornia/* --no-deps\n!mkdir -p /root/.cache/torch/hub/checkpoints\n!cp /kaggle/input/aliked/pytorch/aliked-n16/1/aliked-n16.pth /root/.cache/torch/hub/checkpoints/\n!cp /kaggle/input/lightglue/pytorch/aliked/1/aliked_lightglue.pth /root/.cache/torch/hub/checkpoints/\n!cp /kaggle/input/lightglue/pytorch/aliked/1/aliked_lightglue.pth /root/.cache/torch/hub/checkpoints/aliked_lightglue_v0-1_arxiv-pth","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-07-12T10:24:50.222278Z","iopub.execute_input":"2025-07-12T10:24:50.222587Z","iopub.status.idle":"2025-07-12T10:24:57.172301Z","shell.execute_reply.started":"2025-07-12T10:24:50.222562Z","shell.execute_reply":"2025-07-12T10:24:57.171175Z"}},"outputs":[{"name":"stdout","text":"Processing /kaggle/input/imc2024-packages-lightglue-rerun-kornia/kornia-0.7.2-py2.py3-none-any.whl\nProcessing /kaggle/input/imc2024-packages-lightglue-rerun-kornia/kornia_moons-0.2.9-py3-none-any.whl\nProcessing /kaggle/input/imc2024-packages-lightglue-rerun-kornia/kornia_rs-0.1.2-cp310-cp310-manylinux_2_17_x86_64.manylinux2014_x86_64.whl\nProcessing /kaggle/input/imc2024-packages-lightglue-rerun-kornia/lightglue-0.0-py3-none-any.whl\nProcessing /kaggle/input/imc2024-packages-lightglue-rerun-kornia/pycolmap-0.6.1-cp310-cp310-manylinux_2_17_x86_64.manylinux2014_x86_64.whl\nProcessing /kaggle/input/imc2024-packages-lightglue-rerun-kornia/rerun_sdk-0.15.0a2-cp38-abi3-manylinux_2_31_x86_64.whl\nInstalling collected packages: rerun-sdk, pycolmap, lightglue, kornia-rs, kornia-moons, kornia\n  Attempting uninstall: kornia-rs\n    Found existing installation: kornia_rs 0.1.8\n    Uninstalling kornia_rs-0.1.8:\n      Successfully uninstalled kornia_rs-0.1.8\n  Attempting uninstall: kornia\n    Found existing installation: kornia 0.8.0\n    Uninstalling kornia-0.8.0:\n      Successfully uninstalled kornia-0.8.0\nSuccessfully installed kornia-0.7.2 kornia-moons-0.2.9 kornia-rs-0.1.2 lightglue-0.0 pycolmap-0.6.1 rerun-sdk-0.15.0a2\n","output_type":"stream"}],"execution_count":1},{"cell_type":"code","source":"import sys\nimport os\nfrom tqdm import tqdm\nfrom time import time, sleep\nimport gc\nimport numpy as np\nimport h5py\nimport dataclasses\nimport pandas as pd\nfrom IPython.display import clear_output\nfrom collections import defaultdict\nfrom copy import deepcopy\nfrom PIL import Image\n\nimport cv2\nimport torch\nimport torch.nn.functional as F\nimport kornia as K\nimport kornia.feature as KF\n\nimport torch\nfrom lightglue import match_pair\nfrom lightglue import ALIKED, LightGlue\nfrom lightglue.utils import load_image, rbd\nfrom transformers import AutoImageProcessor, AutoModel\n\n# Utilities: importing data into colmap and competition metric\nimport pycolmap\nsys.path.append('/kaggle/input/imc25-utils')\nfrom database import *\nfrom h5_to_db import *\nimport metric","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-07-12T10:24:57.173735Z","iopub.execute_input":"2025-07-12T10:24:57.173990Z","iopub.status.idle":"2025-07-12T10:25:29.219535Z","shell.execute_reply.started":"2025-07-12T10:24:57.173969Z","shell.execute_reply":"2025-07-12T10:25:29.218566Z"}},"outputs":[{"name":"stderr","text":"/usr/local/lib/python3.10/dist-packages/kornia/feature/lightglue.py:44: FutureWarning: `torch.cuda.amp.custom_fwd(args...)` is deprecated. Please use `torch.amp.custom_fwd(args..., device_type='cuda')` instead.\n  @torch.cuda.amp.custom_fwd(cast_inputs=torch.float32)\n/usr/local/lib/python3.10/dist-packages/lightglue/lightglue.py:24: FutureWarning: `torch.cuda.amp.custom_fwd(args...)` is deprecated. Please use `torch.amp.custom_fwd(args..., device_type='cuda')` instead.\n  @torch.cuda.amp.custom_fwd(cast_inputs=torch.float32)\n","output_type":"stream"}],"execution_count":2},{"cell_type":"code","source":"# Don't forget to select an accelerator on the sidebar to the right.\ndevice = K.utils.get_cuda_device_if_available(0)\nprint(f'{device=}')","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-07-12T10:25:29.220965Z","iopub.execute_input":"2025-07-12T10:25:29.221474Z","iopub.status.idle":"2025-07-12T10:25:29.311981Z","shell.execute_reply.started":"2025-07-12T10:25:29.221451Z","shell.execute_reply":"2025-07-12T10:25:29.310967Z"}},"outputs":[{"name":"stdout","text":"device=device(type='cuda', index=0)\n","output_type":"stream"}],"execution_count":3},{"cell_type":"code","source":"def load_torch_image(fname, device=torch.device('cpu')):\n    img = K.io.load_image(fname, K.io.ImageLoadType.RGB32, device=device)[None, ...]\n    return img\n\n\n# Use efficientnet global descriptor to get matching shortlists.\ndef get_global_desc(fnames, device = torch.device('cpu')):\n    processor = AutoImageProcessor.from_pretrained('/kaggle/input/dinov2/pytorch/base/1')\n    model = AutoModel.from_pretrained('/kaggle/input/dinov2/pytorch/base/1')\n    model = model.eval()\n    model = model.to(device)\n    global_descs_dinov2 = []\n    for i, img_fname_full in tqdm(enumerate(fnames),total= len(fnames)):\n        key = os.path.splitext(os.path.basename(img_fname_full))[0]\n        timg = load_torch_image(img_fname_full)\n        with torch.inference_mode():\n            inputs = processor(images=timg, return_tensors=\"pt\", do_rescale=False).to(device)\n            outputs = model(**inputs)\n            dino_mac = F.normalize(outputs.last_hidden_state[:,1:].max(dim=1)[0], dim=1, p=2)\n        global_descs_dinov2.append(dino_mac.detach().cpu())\n    global_descs_dinov2 = torch.cat(global_descs_dinov2, dim=0)\n    return global_descs_dinov2\n\n\ndef get_img_pairs_exhaustive(img_fnames):\n    index_pairs = []\n    for i in range(len(img_fnames)):\n        for j in range(i+1, len(img_fnames)):\n            index_pairs.append((i,j))\n    return index_pairs\n\n\ndef get_image_pairs_shortlist(fnames,\n                              sim_th = 0.6, # should be strict\n                              min_pairs = 20,\n                              exhaustive_if_less = 20,\n                              device=torch.device('cpu')):\n    num_imgs = len(fnames)\n    if num_imgs <= exhaustive_if_less:\n        return get_img_pairs_exhaustive(fnames)\n    descs = get_global_desc(fnames, device=device)\n    dm = torch.cdist(descs, descs, p=2).detach().cpu().numpy()\n    # removing half\n    mask = dm <= sim_th\n    total = 0\n    matching_list = []\n    ar = np.arange(num_imgs)\n    already_there_set = []\n    for st_idx in range(num_imgs-1):\n        mask_idx = mask[st_idx]\n        to_match = ar[mask_idx]\n        if len(to_match) < min_pairs:\n            to_match = np.argsort(dm[st_idx])[:min_pairs]  \n        for idx in to_match:\n            if st_idx == idx:\n                continue\n            if dm[st_idx, idx] < 1000:\n                matching_list.append(tuple(sorted((st_idx, idx.item()))))\n                total+=1\n    matching_list = sorted(list(set(matching_list)))\n    return matching_list\n\ndef detect_aliked(img_fnames,\n                  feature_dir = '.featureout',\n                  num_features = 4096,\n                  resize_to = 1024,\n                  device=torch.device('cpu')):\n    dtype = torch.float32 # ALIKED has issues with float16\n    extractor = ALIKED(max_num_keypoints=num_features, detection_threshold=0.01, resize=resize_to).eval().to(device, dtype)\n    if not os.path.isdir(feature_dir):\n        os.makedirs(feature_dir)\n    with h5py.File(f'{feature_dir}/keypoints.h5', mode='w') as f_kp, \\\n         h5py.File(f'{feature_dir}/descriptors.h5', mode='w') as f_desc:\n        for img_path in tqdm(img_fnames):\n            img_fname = img_path.split('/')[-1]\n            key = img_fname\n            with torch.inference_mode():\n                image0 = load_torch_image(img_path, device=device).to(dtype)\n                feats0 = extractor.extract(image0)  # auto-resize the image, disable with resize=None\n                kpts = feats0['keypoints'].reshape(-1, 2).detach().cpu().numpy()\n                descs = feats0['descriptors'].reshape(len(kpts), -1).detach().cpu().numpy()\n                f_kp[key] = kpts\n                f_desc[key] = descs\n    return\n\ndef match_with_lightglue(img_fnames,\n                   index_pairs,\n                   feature_dir = '.featureout',\n                   device=torch.device('cpu'),\n                   min_matches=15,verbose=True):\n    lg_matcher = KF.LightGlueMatcher(\"aliked\", {\"width_confidence\": -1,\n                                                \"depth_confidence\": -1,\n                                                 \"mp\": True if 'cuda' in str(device) else False}).eval().to(device)\n    with h5py.File(f'{feature_dir}/keypoints.h5', mode='r') as f_kp, \\\n        h5py.File(f'{feature_dir}/descriptors.h5', mode='r') as f_desc, \\\n        h5py.File(f'{feature_dir}/matches.h5', mode='w') as f_match:\n        for pair_idx in tqdm(index_pairs):\n            idx1, idx2 = pair_idx\n            fname1, fname2 = img_fnames[idx1], img_fnames[idx2]\n            key1, key2 = fname1.split('/')[-1], fname2.split('/')[-1]\n            kp1 = torch.from_numpy(f_kp[key1][...]).to(device)\n            kp2 = torch.from_numpy(f_kp[key2][...]).to(device)\n            desc1 = torch.from_numpy(f_desc[key1][...]).to(device)\n            desc2 = torch.from_numpy(f_desc[key2][...]).to(device)\n            with torch.inference_mode():\n                dists, idxs = lg_matcher(desc1,\n                                         desc2,\n                                         KF.laf_from_center_scale_ori(kp1[None]),\n                                         KF.laf_from_center_scale_ori(kp2[None]))\n            if len(idxs)  == 0:\n                continue\n            n_matches = len(idxs)\n            if verbose:\n                print (f'{key1}-{key2}: {n_matches} matches')\n            group  = f_match.require_group(key1)\n            if n_matches >= min_matches:\n                 group.create_dataset(key2, data=idxs.detach().cpu().numpy().reshape(-1, 2))\n    return\n\ndef import_into_colmap(img_dir, feature_dir ='.featureout', database_path = 'colmap.db'):\n    db = COLMAPDatabase.connect(database_path)\n    db.create_tables()\n    single_camera = False\n    fname_to_id = add_keypoints(db, feature_dir, img_dir, '', 'simple-pinhole', single_camera)\n    add_matches(\n        db,\n        feature_dir,\n        fname_to_id,\n    )\n    db.commit()\n    return","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-07-12T10:25:29.313505Z","iopub.execute_input":"2025-07-12T10:25:29.313776Z","iopub.status.idle":"2025-07-12T10:25:29.353455Z","shell.execute_reply.started":"2025-07-12T10:25:29.313753Z","shell.execute_reply":"2025-07-12T10:25:29.352598Z"}},"outputs":[],"execution_count":4},{"cell_type":"code","source":"# Collect info from the dataset\n\n@dataclasses.dataclass\nclass Prediction:\n    image_id: str | None  # A unique identifier for the row -- unused otherwise. Used only on the hidden test set.\n    dataset: str\n    filename: str\n    cluster_index: int | None = None\n    rotation: np.ndarray | None = None\n    translation: np.ndarray | None = None\n\n# Set is_train=True to run the notebook on the training data.\n# Set is_train=False if submitting an entry to the competition (test data is hidden, and different from what you see on the \"test\" folder).\nis_train = False\ndata_dir = '/kaggle/input/image-matching-challenge-2025'\nworkdir = '/kaggle/working/result/'\nos.makedirs(workdir, exist_ok=True)\n\nif is_train:\n    sample_submission_csv = os.path.join(data_dir, 'train_labels.csv')\nelse:\n    sample_submission_csv = os.path.join(data_dir, 'sample_submission.csv')\n\nsamples = {}\ncompetition_data = pd.read_csv(sample_submission_csv)\nfor _, row in competition_data.iterrows():\n    # Note: For the test data, the \"scene\" column has no meaning, and the rotation_matrix and translation_vector columns are random.\n    if row.dataset not in samples:\n        samples[row.dataset] = []\n    samples[row.dataset].append(\n        Prediction(\n            image_id=None if is_train else row.image_id,\n            dataset=row.dataset,\n            filename=row.image\n        )\n    )\n\nfor dataset in samples:\n    print(f'Dataset \"{dataset}\" -> num_images={len(samples[dataset])}')","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-07-12T10:25:29.354283Z","iopub.execute_input":"2025-07-12T10:25:29.354503Z","iopub.status.idle":"2025-07-12T10:25:29.538789Z","shell.execute_reply.started":"2025-07-12T10:25:29.354484Z","shell.execute_reply":"2025-07-12T10:25:29.538027Z"}},"outputs":[{"name":"stdout","text":"Dataset \"ETs\" -> num_images=22\nDataset \"amy_gardens\" -> num_images=200\nDataset \"fbk_vineyard\" -> num_images=163\nDataset \"imc2023_haiper\" -> num_images=54\nDataset \"imc2023_heritage\" -> num_images=209\nDataset \"imc2023_theather_imc2024_church\" -> num_images=76\nDataset \"imc2024_dioscuri_baalshamin\" -> num_images=138\nDataset \"imc2024_lizard_pond\" -> num_images=214\nDataset \"pt_brandenburg_british_buckingham\" -> num_images=225\nDataset \"pt_piazzasanmarco_grandplace\" -> num_images=168\nDataset \"pt_sacrecoeur_trevi_tajmahal\" -> num_images=225\nDataset \"pt_stpeters_stpauls\" -> num_images=200\nDataset \"stairs\" -> num_images=51\n","output_type":"stream"}],"execution_count":5},{"cell_type":"code","source":"gc.collect()\n\nmax_images = None  # For debugging only. Set to None to disable.\ndatasets_to_process = None  # Not the best convention, but None means all datasets.\n\nif is_train:\n    # max_images = 5\n\n    # Note: When running on the training dataset, the notebook will hit the time limit and die. Use this filter to run on a few specific datasets.\n    datasets_to_process = [\n    \t# New data.\n    \t'amy_gardens',\n    \t'ETs',\n    \t'fbk_vineyard',\n    \t'stairs',\n    \t# Data from IMC 2023 and 2024.\n    \t# 'imc2024_dioscuri_baalshamin',\n    \t# 'imc2023_theather_imc2024_church',\n    \t# 'imc2023_heritage',\n    \t# 'imc2023_haiper',\n    \t# 'imc2024_lizard_pond',\n    \t# Crowdsourced PhotoTourism data.\n    \t# 'pt_stpeters_stpauls',\n    \t# 'pt_brandenburg_british_buckingham',\n    \t# 'pt_piazzasanmarco_grandplace',\n    \t# 'pt_sacrecoeur_trevi_tajmahal',\n    ]\n\ntimings = {\n    \"shortlisting\":[],\n    \"feature_detection\": [],\n    \"feature_matching\":[],\n    \"RANSAC\": [],\n    \"Reconstruction\": [],\n}\nmapping_result_strs = []\n\n\nprint (f\"Extracting on device {device}\")\nfor dataset, predictions in samples.items():\n    if datasets_to_process and dataset not in datasets_to_process:\n        print(f'Skipping \"{dataset}\"')\n        continue\n    \n    images_dir = os.path.join(data_dir, 'train' if is_train else 'test', dataset)\n    images = [os.path.join(images_dir, p.filename) for p in predictions]\n    if max_images is not None:\n        images = images[:max_images]\n\n    print(f'\\nProcessing dataset \"{dataset}\": {len(images)} images')\n\n    filename_to_index = {p.filename: idx for idx, p in enumerate(predictions)}\n\n    feature_dir = os.path.join(workdir, 'featureout', dataset)\n    os.makedirs(feature_dir, exist_ok=True)\n\n    # Wrap algos in try-except blocks so we can populate a submission even if one scene crashes.\n    try:\n        t = time()\n        index_pairs = get_image_pairs_shortlist(\n            images,\n            sim_th = 0.3, # should be strict\n            min_pairs = 20, # we select at least min_pairs PER IMAGE with biggest similarity\n            exhaustive_if_less = 20,\n            device=device\n        )\n        timings['shortlisting'].append(time() - t)\n        print (f'Shortlisting. Number of pairs to match: {len(index_pairs)}. Done in {time() - t:.4f} sec')\n        gc.collect()\n    \n        t = time()\n\n        detect_aliked(images, feature_dir, 4096, device=device)\n        gc.collect()\n        timings['feature_detection'].append(time() - t)\n        print(f'Features detected in {time() - t:.4f} sec')\n        \n        t = time()\n        match_with_lightglue(images, index_pairs, feature_dir=feature_dir, device=device, verbose=False)\n        timings['feature_matching'].append(time() - t)\n        print(f'Features matched in {time() - t:.4f} sec')\n\n        database_path = os.path.join(feature_dir, 'colmap.db')\n        if os.path.isfile(database_path):\n            os.remove(database_path)\n        gc.collect()\n        sleep(1)\n        import_into_colmap(images_dir, feature_dir=feature_dir, database_path=database_path)\n        output_path = f'{feature_dir}/colmap_rec_aliked'\n        \n        t = time()\n        pycolmap.match_exhaustive(database_path)\n        timings['RANSAC'].append(time() - t)\n        print(f'Ran RANSAC in {time() - t:.4f} sec')\n        \n        # By default colmap does not generate a reconstruction if less than 10 images are registered.\n        # Lower it to 3.\n        mapper_options = pycolmap.IncrementalPipelineOptions()\n        mapper_options.min_model_size = 3\n        mapper_options.max_num_models = 25\n        os.makedirs(output_path, exist_ok=True)\n        t = time()\n        maps = pycolmap.incremental_mapping(\n            database_path=database_path, \n            image_path=images_dir,\n            output_path=output_path,\n            options=mapper_options)\n        sleep(1)\n        timings['Reconstruction'].append(time() - t)\n        print(f'Reconstruction done in  {time() - t:.4f} sec')\n        print(maps)\n\n        clear_output(wait=False)\n    \n        registered = 0\n        for map_index, cur_map in maps.items():\n            for index, image in cur_map.images.items():\n                prediction_index = filename_to_index[image.name]\n                predictions[prediction_index].cluster_index = map_index\n                predictions[prediction_index].rotation = deepcopy(image.cam_from_world.rotation.matrix())\n                predictions[prediction_index].translation = deepcopy(image.cam_from_world.translation)\n                registered += 1\n        mapping_result_str = f'Dataset \"{dataset}\" -> Registered {registered} / {len(images)} images with {len(maps)} clusters'\n        mapping_result_strs.append(mapping_result_str)\n        print(mapping_result_str)\n        gc.collect()\n    except Exception as e:\n        print(e)\n        # raise e\n        mapping_result_str = f'Dataset \"{dataset}\" -> Failed!'\n        mapping_result_strs.append(mapping_result_str)\n        print(mapping_result_str)\n\nprint('\\nResults')\nfor s in mapping_result_strs:\n    print(s)\n\nprint('\\nTimings')\nfor k, v in timings.items():\n    print(f'{k} -> total={sum(v):.02f} sec.')","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-07-12T10:25:29.539733Z","iopub.execute_input":"2025-07-12T10:25:29.539976Z","iopub.status.idle":"2025-07-12T10:29:49.141182Z","shell.execute_reply.started":"2025-07-12T10:25:29.539955Z","shell.execute_reply":"2025-07-12T10:29:49.140446Z"}},"outputs":[{"name":"stdout","text":"Dataset \"stairs\" -> Registered 55 / 51 images with 3 clusters\n\nResults\nDataset \"ETs\" -> Registered 21 / 22 images with 1 clusters\nDataset \"amy_gardens\" -> Failed!\nDataset \"fbk_vineyard\" -> Failed!\nDataset \"imc2023_haiper\" -> Failed!\nDataset \"imc2023_heritage\" -> Failed!\nDataset \"imc2023_theather_imc2024_church\" -> Failed!\nDataset \"imc2024_dioscuri_baalshamin\" -> Failed!\nDataset \"imc2024_lizard_pond\" -> Failed!\nDataset \"pt_brandenburg_british_buckingham\" -> Failed!\nDataset \"pt_piazzasanmarco_grandplace\" -> Failed!\nDataset \"pt_sacrecoeur_trevi_tajmahal\" -> Failed!\nDataset \"pt_stpeters_stpauls\" -> Failed!\nDataset \"stairs\" -> Registered 55 / 51 images with 3 clusters\n\nTimings\nshortlisting -> total=21.58 sec.\nfeature_detection -> total=7.36 sec.\nfeature_matching -> total=107.90 sec.\nRANSAC -> total=8.89 sec.\nReconstruction -> total=103.72 sec.\n","output_type":"stream"}],"execution_count":6},{"cell_type":"code","source":"# Create a submission file.\n\narray_to_str = lambda array: ';'.join([f\"{x:.09f}\" for x in array])\nnone_to_str = lambda n: ';'.join(['nan'] * n)\n\nsubmission_file = '/kaggle/working/submission.csv'\nwith open(submission_file, 'w') as f:\n    if is_train:\n        f.write('dataset,scene,image,rotation_matrix,translation_vector\\n')\n        for dataset in samples:\n            for prediction in samples[dataset]:\n                cluster_name = 'outliers' if prediction.cluster_index is None else f'cluster{prediction.cluster_index}'\n                rotation = none_to_str(9) if prediction.rotation is None else array_to_str(prediction.rotation.flatten())\n                translation = none_to_str(3) if prediction.translation is None else array_to_str(prediction.translation)\n                f.write(f'{prediction.dataset},{cluster_name},{prediction.filename},{rotation},{translation}\\n')\n    else:\n        f.write('image_id,dataset,scene,image,rotation_matrix,translation_vector\\n')\n        for dataset in samples:\n            for prediction in samples[dataset]:\n                cluster_name = 'outliers' if prediction.cluster_index is None else f'cluster{prediction.cluster_index}'\n                rotation = none_to_str(9) if prediction.rotation is None else array_to_str(prediction.rotation.flatten())\n                translation = none_to_str(3) if prediction.translation is None else array_to_str(prediction.translation)\n                f.write(f'{prediction.image_id},{prediction.dataset},{cluster_name},{prediction.filename},{rotation},{translation}\\n')\n\n!head {submission_file}","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-07-12T10:29:49.142098Z","iopub.execute_input":"2025-07-12T10:29:49.142379Z","iopub.status.idle":"2025-07-12T10:29:49.317870Z","shell.execute_reply.started":"2025-07-12T10:29:49.142356Z","shell.execute_reply":"2025-07-12T10:29:49.317025Z"}},"outputs":[{"name":"stdout","text":"image_id,dataset,scene,image,rotation_matrix,translation_vector\nETs_another_et_another_et001.png_public,ETs,cluster0,another_et_another_et001.png,0.999800777;-0.003973104;0.019560673;0.003576425;0.999788088;0.020272831;-0.019637074;-0.020198835;0.999603117,-2.066643869;-1.589019150;2.083108070\nETs_another_et_another_et002.png_public,ETs,cluster0,another_et_another_et002.png,0.999988019;-0.001852306;0.004531118;0.001866505;0.999993356;-0.003131242;-0.004525287;0.003139662;0.999984832,-1.904556653;-0.811900519;0.610323547\nETs_another_et_another_et003.png_public,ETs,cluster0,another_et_another_et003.png,0.997609208;-0.041471461;0.055280969;0.044799872;0.997168668;-0.060395520;-0.052619760;0.062727708;0.996642562,-2.149261095;0.729616374;-1.114843099\nETs_another_et_another_et004.png_public,ETs,cluster0,another_et_another_et004.png,0.999081985;-0.011363544;0.041304438;0.005905654;0.991505276;0.129932334;-0.042430060;-0.129569124;0.990662168,-2.039075669;-1.146665605;-0.739285854\nETs_another_et_another_et005.png_public,ETs,cluster0,another_et_another_et005.png,0.994891948;0.002863173;0.100904973;-0.009249887;0.997978010;0.062883469;-0.100520898;-0.063495617;0.992906771,-2.587184809;-1.796041615;0.638064678\nETs_another_et_another_et006.png_public,ETs,cluster0,another_et_another_et006.png,0.922824802;0.193595806;-0.333039110;-0.219402948;0.974758938;-0.041320192;0.316633433;0.111201060;0.942007215,0.322669563;-0.529359411;0.787793287\nETs_another_et_another_et007.png_public,ETs,cluster0,another_et_another_et007.png,0.785392546;0.264583080;-0.559601950;-0.311313703;0.950226963;0.012348983;0.535016194;0.164512957;0.828669512,2.042109973;-0.341215189;0.582324089\nETs_another_et_another_et008.png_public,ETs,cluster0,another_et_another_et008.png,0.575123410;0.311903388;-0.756273324;-0.394581969;0.915583348;0.077538396;0.716615750;0.253817671;0.649644870,3.674366604;-0.762181588;1.610196902\nETs_another_et_another_et009.png_public,ETs,cluster0,another_et_another_et009.png,0.329271790;0.347601732;-0.877925466;-0.490918060;0.857253170;0.155294753;0.806585114;0.379855286;0.452913254,5.190043807;-1.209787997;2.465526803\n","output_type":"stream"}],"execution_count":7},{"cell_type":"code","source":"# Compute results if running on the training set.\n# Don't do this when submitting a notebook for scoring. All you have to do is save your submission to /kaggle/working/submission.csv.\n\nif is_train:\n    t = time()\n    final_score, dataset_scores = metric.score(\n        gt_csv='/kaggle/input/image-matching-challenge-2025/train_labels.csv',\n        user_csv=submission_file,\n        thresholds_csv='/kaggle/input/image-matching-challenge-2025/train_thresholds.csv',\n        mask_csv=None if is_train else os.path.join(data_dir, 'mask.csv'),\n        inl_cf=0,\n        strict_cf=-1,\n        verbose=True,\n    )\n    print(f'Computed metric in: {time() - t:.02f} sec.')","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-07-12T10:29:49.320022Z","iopub.execute_input":"2025-07-12T10:29:49.320317Z","iopub.status.idle":"2025-07-12T10:29:49.324926Z","shell.execute_reply.started":"2025-07-12T10:29:49.320291Z","shell.execute_reply":"2025-07-12T10:29:49.324112Z"}},"outputs":[],"execution_count":8}]}